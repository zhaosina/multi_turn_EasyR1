#!/usr/bin/env python3
"""
åŸºäºQwen-VL2.5çš„è¿­ä»£å¼groundingç³»ç»Ÿ
å®ç°è‡ªæˆ‘çº æ­£çš„è§†è§‰å®šä½åŠŸèƒ½
"""

import torch
from PIL import Image, ImageDraw, ImageFont
import re
import os
import warnings
warnings.filterwarnings('ignore')

try:
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    print("âœ… æˆåŠŸå¯¼å…¥ Qwen2.5-VL ç›¸å…³ç»„ä»¶")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥Qwen2.5-VLç»„ä»¶: {e}")
    print("è¯·å®‰è£…: pip install git+https://github.com/huggingface/transformers accelerate")
    raise

try:
    from qwen_vl_utils import process_vision_info
    print("âœ… æˆåŠŸå¯¼å…¥ qwen_vl_utils")
    HAS_QWEN_VL_UTILS = True
except ImportError:
    print("âš ï¸ qwen_vl_utils ä¸å¯ç”¨ï¼Œä½¿ç”¨å†…ç½®å¤„ç†æ–¹æ³•")
    HAS_QWEN_VL_UTILS = False

class QwenVL25IterativeGrounding:
    def __init__(self, model_path='Qwen/Qwen2.5-VL-7B-Instruct'):
        """åˆå§‹åŒ–Qwen-VL2.5æ¨¡å‹"""
        print("ğŸš€ åŠ è½½Qwen-VL2.5æ¨¡å‹...")
        self.model_path = model_path
        self.model = None
        self.processor = None
        
        success = self._load_model()
        if not success:
            raise RuntimeError("âŒ æ— æ³•åŠ è½½Qwen-VL2.5æ¨¡å‹")

    def _load_model(self):
        """åŠ è½½Qwen-VL2.5æ¨¡å‹"""
        try:
            print(f"ğŸ”„ ä» {self.model_path} åŠ è½½æ¨¡å‹...")
            
            # åŠ è½½æ¨¡å‹
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype="auto",
                device_map="auto"
            )
            
            # åŠ è½½å¤„ç†å™¨
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            
            print("âœ… Qwen-VL2.5æ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"ğŸ“Š æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            print("ğŸ”§ è¯·æ£€æŸ¥:")
            print("   1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("   2. æ˜¯å¦å·²å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„transformers: pip install git+https://github.com/huggingface/transformers accelerate")
            print("   3. æ˜¯å¦å·²å®‰è£…qwen-vl-utils: pip install qwen-vl-utils")
            print("   4. æ˜¯å¦æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
            return False

    def _process_vision_info(self, messages):
        """å¤„ç†è§†è§‰ä¿¡æ¯"""
        if HAS_QWEN_VL_UTILS:
            try:
                return process_vision_info(messages)
            except Exception as e:
                print(f"âš ï¸ qwen_vl_utilså¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨å†…ç½®æ–¹æ³•")
        
        # å†…ç½®å¤„ç†æ–¹æ³•
        image_inputs = []
        video_inputs = []
        
        for message in messages:
            if isinstance(message.get('content'), list):
                for content in message['content']:
                    if content.get('type') == 'image':
                        image_path = content.get('image')
                        if image_path and os.path.exists(image_path):
                            image_inputs.append(Image.open(image_path).convert('RGB'))
        
        return image_inputs, video_inputs

    def get_prediction(self, image_path, task_instruction, iteration=0, feedback=""):
        """è·å–æ¨¡å‹é¢„æµ‹"""
        
        if iteration == 0:
            # åˆæ¬¡é¢„æµ‹ä½¿ç”¨æ ‡å‡†prompt
            sys_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†è§‰å®šä½åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå±å¹•æˆªå›¾å¹¶é¢„æµ‹æœ€ä½³çš„ç‚¹å‡»ä½ç½®ã€‚

åŸºæœ¬åŠ¨ä½œè§„èŒƒ:
- CLICK: ç‚¹å‡»æŒ‡å®šä½ç½®
- æ ¼å¼: CLICK <point>[[xåæ ‡, yåæ ‡]]</point>
- ç¤ºä¾‹: CLICK <point>[[101, 872]]</point>

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”:
æ€è€ƒ: è¯¦ç»†è¯´æ˜ä½ çš„æ¨ç†è¿‡ç¨‹
åŠ¨ä½œ: å…·ä½“çš„ç‚¹å‡»åŠ¨ä½œ

ä»»åŠ¡æŒ‡ä»¤: """
            
            full_prompt = sys_prompt + task_instruction
        else:
            # çº æ­£æ—¶çš„prompt
            try:
                img = Image.open(image_path)
                img_width, img_height = img.size
            except:
                img_width, img_height = 1080, 2400  # é»˜è®¤å°ºå¯¸
                
            sys_prompt = f"""è¿™æ˜¯ç¬¬{iteration + 1}æ¬¡ä½ç½®é¢„æµ‹ã€‚ä½ çš„å‰ä¸€æ¬¡é¢„æµ‹è¢«éªŒè¯ä¸ºä¸å‡†ç¡®ï¼Œç°åœ¨éœ€è¦é‡æ–°åˆ†æå¹¶é¢„æµ‹ä¸€ä¸ªå®Œå…¨ä¸åŒçš„ä½ç½®ã€‚

**å›¾åƒä¿¡æ¯:**
- å›¾åƒå°ºå¯¸: {img_width} x {img_height} åƒç´ 
- çº¢è‰²åœ†åœˆæ ‡è®°äº†ä½ ä¸Šä¸€æ¬¡çš„é”™è¯¯é¢„æµ‹ä½ç½®

**å…³é”®æŒ‡ç¤º:**
1. ä½ ä¸Šä¸€æ¬¡çš„é¢„æµ‹æ˜¯é”™è¯¯çš„ï¼Œå¿…é¡»é¿å…å†æ¬¡é¢„æµ‹ç›¸åŒæˆ–ç›¸è¿‘çš„ä½ç½®
2. ä»”ç»†è§‚å¯Ÿå›¾åƒä¸­çš„çº¢åœˆï¼Œé‚£ä¸ªä½ç½®æ˜¯é”™è¯¯çš„
3. é‡æ–°åˆ†ææ•´ä¸ªç•Œé¢ï¼Œå¯»æ‰¾æ­£ç¡®çš„ç›®æ ‡å…ƒç´ 
4. é¢„æµ‹ä¸€ä¸ªä¸çº¢åœˆä½ç½®æ˜æ˜¾ä¸åŒçš„æ–°åæ ‡

**åé¦ˆä¿¡æ¯:**
{feedback}

**è¦æ±‚:**
- å¿…é¡»é¢„æµ‹ä¸ä¹‹å‰ä¸åŒçš„åæ ‡ä½ç½®
- ä»”ç»†åˆ†æç›®æ ‡å…ƒç´ çš„å®é™…ä½ç½®
- ä¸è¦é‡å¤ä¹‹å‰çš„é”™è¯¯

åŠ¨ä½œæ ¼å¼: CLICK <point>[[xåæ ‡, yåæ ‡]]</point>

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”:
æ€è€ƒ: åˆ†æä¸ºä»€ä¹ˆä¸Šæ¬¡é¢„æµ‹é”™è¯¯ï¼Œä»¥åŠå¦‚ä½•æ‰¾åˆ°æ­£ç¡®ä½ç½® DPO æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæœ‰ç›‘ç£çš„å¾®è°ƒè¿‡ç¨‹ï¼ˆä½¿ç”¨ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼‰ï¼Œå…¶ç¨³å®šæ€§å’Œå¯å¤ç°æ€§è¿œé«˜äºåœ¨çº¿ RLã€‚
åŠ¨ä½œ: æä¾›ä¸€ä¸ªå…¨æ–°çš„ç‚¹å‡»ä½ç½®

ä»»åŠ¡æŒ‡ä»¤: """
            
            full_prompt = sys_prompt + task_instruction + f"\nä½ç½®åˆ†æå’Œçº æ­£: {feedback}"

        # æ„å»ºæ¶ˆæ¯æ ¼å¼
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": full_prompt},
                ],
            }
        ]

        try:
            # å¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = self._process_vision_info(messages)
            
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )

            # ç§»åˆ°æ­£ç¡®è®¾å¤‡
            if torch.cuda.is_available():
                inputs = inputs.to("cuda")

            # ç”Ÿæˆé¢„æµ‹ - ä¸ºè¿­ä»£å¢åŠ éšæœºæ€§
            with torch.no_grad():
                if iteration == 0:
                    # é¦–æ¬¡é¢„æµ‹ä½¿ç”¨è¾ƒä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
                    generated_ids = self.model.generate(
                        **inputs, 
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.3,
                        top_p=0.9
                    )
                else:
                    # åç»­è¿­ä»£ä½¿ç”¨æ›´é«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§
                    generated_ids = self.model.generate(
                        **inputs, 
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.8,
                        top_p=0.95,
                        repetition_penalty=1.1
                    )

            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )
            
            return output_text[0] if isinstance(output_text, list) else output_text
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
            return "æ€è€ƒ: æ— æ³•å¤„ç†å›¾åƒï¼Œä½¿ç”¨é»˜è®¤ä½ç½®ã€‚\nåŠ¨ä½œ: CLICK <point>[[400, 400]]</point>"

    def extract_coordinates(self, text):
        """ä»æ–‡æœ¬ä¸­æå–åæ ‡"""
        # å¤šç§åæ ‡æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            r'CLICK <point>\[\[(\d+),\s*(\d+)\]\]</point>',  # æ ‡å‡†æ ¼å¼
            r'<point>\[\[(\d+),\s*(\d+)\]\]</point>',        # ç®€åŒ–æ ¼å¼1
            r'\[\[(\d+),\s*(\d+)\]\]',                        # ç®€åŒ–æ ¼å¼2
            r'\((\d+),\s*(\d+)\)',                            # æ‹¬å·æ ¼å¼
            r'(\d+),\s*(\d+)'                                 # çº¯æ•°å­—å¯¹
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            if matches:
                # å¯¹äºæœ€åä¸€ä¸ªæ¨¡å¼ï¼Œéœ€è¦éªŒè¯æ•°å­—æ˜¯å¦åˆç†
                if pattern == r'(\d+),\s*(\d+)':
                    for x_str, y_str in matches:
                        x, y = int(x_str), int(y_str)
                        if 0 <= x <= 2000 and 0 <= y <= 3000:  # åˆç†çš„å±å¹•åæ ‡èŒƒå›´
                            return x, y
                else:
                    x, y = int(matches[0][0]), int(matches[0][1])
                    return x, y
        
        print(f"âš ï¸ æ— æ³•ä»æ–‡æœ¬ä¸­æå–åæ ‡: {text[:200]}...")
        return None, None

    def mark_image(self, image_path, x, y, output_path):
        """åœ¨å›¾åƒä¸Šæ ‡è®°é¢„æµ‹ä½ç½®"""
        img = Image.open(image_path).convert('RGB')
        draw = ImageDraw.Draw(img)
        
        # ç”»çº¢è‰²åœ†åœˆ
        radius = 20
        draw.ellipse((x-radius, y-radius, x+radius, y+radius), outline='red', width=5)
        
        # æ·»åŠ æ ‡ç­¾
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", 24)
        except:
            font = ImageFont.load_default()
        
        draw.text((x+radius+5, y-radius), "é¢„æµ‹", fill='red', font=font)
        
        img.save(output_path)
        print(f"âœ… æ ‡è®°ä½ç½® ({x}, {y}) ä¿å­˜åˆ° {output_path}")
        return img.size

    def verify_position(self, marked_image_path, task_instruction, predicted_x, predicted_y):
        """éªŒè¯é¢„æµ‹ä½ç½®å¹¶æä¾›æ”¹è¿›å»ºè®®"""
        
        verify_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„UIä½ç½®éªŒè¯ä¸“å®¶ã€‚ä½ éœ€è¦éå¸¸ä»”ç»†åœ°éªŒè¯ä¸€ä¸ªç‚¹å‡»ä½ç½®çš„å‡†ç¡®æ€§ã€‚

ä»»åŠ¡: "{task_instruction}"

å›¾åƒä¸­æœ‰ä¸€ä¸ªçº¢è‰²åœ†åœˆæ ‡è®°äº†é¢„æµ‹ä½ç½® ({predicted_x}, {predicted_y})ã€‚

**ä¸¥æ ¼éªŒè¯æ ‡å‡†:**
1. çº¢åœˆå¿…é¡»ç²¾ç¡®è¦†ç›–ç›®æ ‡UIå…ƒç´ çš„ä¸­å¿ƒåŒºåŸŸ
2. å¦‚æœçº¢åœˆåªæ˜¯æ¥è¿‘ç›®æ ‡ä½†æ²¡æœ‰å‡†ç¡®è¦†ç›–ï¼Œåˆ™è§†ä¸ºä¸å‡†ç¡®
3. å¦‚æœç›®æ ‡å…ƒç´ å¾ˆå°ï¼Œçº¢åœˆå¿…é¡»åœ¨å…ƒç´ å†…éƒ¨
4. å¦‚æœç›®æ ‡å…ƒç´ è¾ƒå¤§ï¼Œçº¢åœˆå¿…é¡»åœ¨å…ƒç´ çš„å¯ç‚¹å‡»åŒºåŸŸå†…

**è¯¦ç»†åˆ†æè¦æ±‚:**
- ä»”ç»†è§‚å¯Ÿçº¢åœˆçš„ä½ç½®
- è¯†åˆ«ä»»åŠ¡è¦æ±‚çš„ç›®æ ‡å…ƒç´ åœ¨å“ªé‡Œ
- åˆ¤æ–­çº¢åœˆæ˜¯å¦å‡†ç¡®å‘½ä¸­ç›®æ ‡å…ƒç´ 
- å¦‚æœä¸å‡†ç¡®ï¼Œæ˜ç¡®æŒ‡å‡ºæ­£ç¡®çš„ç›®æ ‡ä½ç½®

**å›ç­”æ ¼å¼ (ä¸¥æ ¼éµå®ˆ):**
å¦‚æœçº¢åœˆå‡†ç¡®å‘½ä¸­ç›®æ ‡å…ƒç´ : "éªŒè¯é€šè¿‡"
å¦‚æœçº¢åœˆæ²¡æœ‰å‡†ç¡®å‘½ä¸­: "éªŒè¯å¤±è´¥ã€‚ç›®æ ‡å…ƒç´ ä½äº: CLICK <point>[[å‡†ç¡®çš„xåæ ‡, å‡†ç¡®çš„yåæ ‡]]</point>"

è¯·è¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œä¸è¦è¿‡äºå®½æ¾ã€‚"""

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": marked_image_path},
                    {"type": "text", "text": verify_prompt},
                ],
            }
        ]

        try:
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = self._process_vision_info(messages)
            
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )

            if torch.cuda.is_available():
                inputs = inputs.to("cuda")

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=200,
                    do_sample=False
                )

            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )
            
            verification_text = output_text[0] if isinstance(output_text, list) else output_text
            
        except Exception as e:
            print(f"âŒ éªŒè¯ä½ç½®æ—¶å‡ºé”™: {e}")
            img = Image.open(marked_image_path)
            img_width, img_height = img.size
            center_x, center_y = img_width // 2, img_height // 2
            verification_text = f"ç”±äºæŠ€æœ¯é—®é¢˜æ— æ³•åˆ†æã€‚å°è¯•ä¸­å¿ƒä½ç½®: CLICK <point>[[{center_x}, {center_y}]]</point>"
        
        # è§£æéªŒè¯ç»“æœ - ä½¿ç”¨æ›´ä¸¥æ ¼çš„åˆ¤æ–­é€»è¾‘
        verify_lower = verification_text.lower()
        
        print(f"ğŸ” éªŒè¯æ¨¡å‹å›å¤: {verification_text}")
        
        # æ›´ä¸¥æ ¼çš„æˆåŠŸåˆ¤æ–­æ¡ä»¶
        success_indicators = ["éªŒè¯é€šè¿‡", "ä½ç½®æ­£ç¡®", "å‡†ç¡®å‘½ä¸­", "correctly positioned"]
        failure_indicators = ["éªŒè¯å¤±è´¥", "ä½ç½®ä¸æ­£ç¡®", "æ²¡æœ‰å‡†ç¡®å‘½ä¸­", "not accurate", "incorrect"]
        
        is_success = False
        is_explicit_failure = False
        
        # æ£€æŸ¥æ˜ç¡®çš„æˆåŠŸæŒ‡ç¤º
        for indicator in success_indicators:
            if indicator in verification_text:
                is_success = True
                break
        
        # æ£€æŸ¥æ˜ç¡®çš„å¤±è´¥æŒ‡ç¤º    
        for indicator in failure_indicators:
            if indicator in verification_text:
                is_explicit_failure = True
                break
        
        # é»˜è®¤ä¸ºå¤±è´¥ï¼Œé™¤éæœ‰æ˜ç¡®çš„æˆåŠŸæŒ‡ç¤º
        if is_success and not is_explicit_failure:
            print(f"âœ… ä½ç½®éªŒè¯é€šè¿‡! ä½ç½®: ({predicted_x}, {predicted_y})")
            return True, predicted_x, predicted_y
        else:
            print(f"âŒ ä½ç½®éªŒè¯å¤±è´¥ï¼Œéœ€è¦è°ƒæ•´...")
            
            # å°è¯•æå–å»ºè®®çš„æ–°ä½ç½®
            suggested_x, suggested_y = predicted_x, predicted_y
            
            # æŸ¥æ‰¾CLICKæ ¼å¼çš„åæ ‡
            click_patterns = [
                r'CLICK <point>\[\[(\d+),\s*(\d+)\]\]</point>',
                r'<point>\[\[(\d+),\s*(\d+)\]\]</point>',
                r'\[\[(\d+),\s*(\d+)\]\]'
            ]
            
            for pattern in click_patterns:
                matches = re.findall(pattern, verification_text)
                if matches:
                    suggested_x, suggested_y = int(matches[0][0]), int(matches[0][1])
                    print(f"ğŸ¯ æ¨¡å‹å»ºè®®æ–°ä½ç½®: ({suggested_x}, {suggested_y})")
                    break
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°å»ºè®®åæ ‡ï¼Œæ ‡è®°ä¸ºæ˜ç¡®å¤±è´¥
                print(f"âš ï¸ æ¨¡å‹æœªæä¾›æ˜ç¡®çš„ä¿®æ­£ä½ç½®ï¼Œå°†é‡æ–°åˆ†æ")
                # ä¸è¦ä½¿ç”¨ä¸­å¿ƒç‚¹ä½œä¸ºé»˜è®¤ï¼Œè€Œæ˜¯ä¿æŒåŸä½ç½®è®©ä¸‹æ¬¡è¿­ä»£å¤„ç†
                suggested_x, suggested_y = predicted_x, predicted_y
            
            return False, suggested_x, suggested_y

    def _is_reasonable_position(self, x, y, image_path):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…ä¸”åˆç†"""
        try:
            img = Image.open(image_path)
            img_width, img_height = img.size
            
            # æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨å›¾åƒèŒƒå›´å†…ï¼Œå¹¶ç•™æœ‰ä¸€å®šè¾¹è·
            margin = 10
            if (margin <= x <= img_width - margin and 
                margin <= y <= img_height - margin):
                return True
            else:
                print(f"âš ï¸ å»ºè®®ä½ç½® ({x}, {y}) è¶…å‡ºå›¾åƒèŒƒå›´ ({img_width}x{img_height})")
                return False
        except Exception as e:
            print(f"âš ï¸ æ— æ³•éªŒè¯ä½ç½®åˆç†æ€§: {e}")
            return False

    def run_iterative_grounding(self, image_path, task_instruction, max_iterations=3):
        """è¿è¡Œè¿­ä»£groundingæµç¨‹"""
        print(f"\nğŸ¯ ä»»åŠ¡: {task_instruction}")
        print("="*50)
        
        original_image = image_path
        current_image = image_path
        feedback = ""
        
        for iteration in range(max_iterations):
            print(f"\nğŸ”„ ç¬¬ {iteration + 1} æ¬¡è¿­ä»£")
            print("-" * 30)
            
            # æ­¥éª¤1: è·å–é¢„æµ‹
            print("ğŸ“ è·å–ä½ç½®é¢„æµ‹...")
            prediction_text = self.get_prediction(current_image, task_instruction, iteration, feedback)
            print(f"æ¨¡å‹å›ç­”: {prediction_text}")
            
            # æå–åæ ‡
            x, y = self.extract_coordinates(prediction_text)
            if x is None or y is None:
                print("âŒ æ— æ³•æå–æœ‰æ•ˆåæ ‡ï¼Œåœæ­¢è¿­ä»£")
                return None
            
            print(f"ğŸ¯ é¢„æµ‹ä½ç½®: ({x}, {y})")
            
            # æ­¥éª¤2: æ ‡è®°å›¾åƒ
            print("ğŸ–ï¸ æ ‡è®°é¢„æµ‹ä½ç½®...")
            marked_path = f"qwenvl25_marked_iter_{iteration + 1}.jpg"
            img_size = self.mark_image(original_image, x, y, marked_path)
            
            # æ­¥éª¤3: éªŒè¯å‡†ç¡®æ€§
            print("âœ… éªŒè¯ä½ç½®å‡†ç¡®æ€§...")
            is_accurate, suggested_x, suggested_y = self.verify_position(marked_path, task_instruction, x, y)
            
            if is_accurate:
                print("ğŸ‰ ä½ç½®å‡†ç¡®ï¼ä»»åŠ¡å®Œæˆã€‚")
                print(f"âœ¨ æœ€ç»ˆä½ç½®: ({suggested_x}, {suggested_y})")
                print(f"ğŸ“ ä½¿ç”¨äº† {iteration + 1} æ¬¡è¿­ä»£")
                return {
                    "success": True,
                    "final_position": (suggested_x, suggested_y),
                    "iterations": iteration + 1,
                    "final_image": marked_path
                }
            else:
                print("âŒ ä½ç½®éªŒè¯å¤±è´¥ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£...")
                
                # åªæœ‰å½“å»ºè®®ä½ç½®ä¸å½“å‰ä½ç½®ä¸åŒä¸”çœ‹èµ·æ¥åˆç†æ—¶ï¼Œæ‰éªŒè¯å»ºè®®ä½ç½®
                if (suggested_x, suggested_y) != (x, y) and self._is_reasonable_position(suggested_x, suggested_y, original_image):
                    print(f"ğŸ” éªŒè¯æ¨¡å‹å»ºè®®çš„ä½ç½®: ({suggested_x}, {suggested_y})")
                    next_marked_path = f"qwenvl25_marked_iter_{iteration + 1}_suggested.jpg"
                    self.mark_image(original_image, suggested_x, suggested_y, next_marked_path)
                    
                    # éªŒè¯å»ºè®®ä½ç½®ï¼Œä½†ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†
                    is_accurate_suggested, _, _ = self.verify_position(next_marked_path, task_instruction, suggested_x, suggested_y)
                    if is_accurate_suggested:
                        print("ğŸ‰ å»ºè®®ä½ç½®éªŒè¯é€šè¿‡ï¼ä»»åŠ¡å®Œæˆã€‚")
                        print(f"âœ¨ æœ€ç»ˆä½ç½®: ({suggested_x}, {suggested_y})")
                        return {
                            "success": True,
                            "final_position": (suggested_x, suggested_y),
                            "iterations": iteration + 1,
                            "final_image": next_marked_path
                        }
                    else:
                        print("âŒ å»ºè®®ä½ç½®ä¹Ÿæœªé€šè¿‡éªŒè¯")
                else:
                    print("âš ï¸ å»ºè®®ä½ç½®æ— æ•ˆæˆ–ä¸å½“å‰ä½ç½®ç›¸åŒï¼Œç»§ç»­è¿­ä»£")
                
                # è®¾ç½®ä¸‹ä¸€æ¬¡è¿­ä»£çš„åé¦ˆ - æä¾›æ›´å…·ä½“çš„æŒ‡å¯¼
                failed_positions = [(x, y)]
                if iteration > 0:
                    # æ”¶é›†ä¹‹å‰æ‰€æœ‰å¤±è´¥çš„ä½ç½®
                    if "å¤±è´¥ä½ç½®å†å²:" in feedback:
                        prev_positions = re.findall(r'\((\d+), (\d+)\)', feedback.split("å¤±è´¥ä½ç½®å†å²:")[1])
                        failed_positions.extend([(int(px), int(py)) for px, py in prev_positions])
                
                failed_pos_str = ", ".join([f"({px}, {py})" for px, py in failed_positions])
                
                if (suggested_x, suggested_y) != (x, y):
                    feedback = f"""ç¬¬{iteration + 1}æ¬¡é¢„æµ‹ ({x}, {y}) éªŒè¯å¤±è´¥ã€‚
éªŒè¯æ¨¡å‹å»ºè®®ä½ç½®: ({suggested_x}, {suggested_y})
å¤±è´¥ä½ç½®å†å²: {failed_pos_str}

é”™è¯¯åˆ†æ: ä½ ä¸€ç›´é¢„æµ‹é”™è¯¯çš„ä½ç½®ã€‚è¯·å®Œå…¨é‡æ–°åˆ†æå›¾åƒï¼Œå¯»æ‰¾ä¸è¿™äº›å¤±è´¥ä½ç½®æ˜æ˜¾ä¸åŒçš„ç›®æ ‡å…ƒç´ ã€‚
æŒ‡å¯¼å»ºè®®: å°è¯•å›¾åƒçš„å…¶ä»–åŒºåŸŸï¼Œä»”ç»†è§‚å¯ŸUIå…ƒç´ çš„å¸ƒå±€å’Œå±‚æ¬¡ç»“æ„ã€‚"""
                else:
                    feedback = f"""ç¬¬{iteration + 1}æ¬¡é¢„æµ‹ ({x}, {y}) éªŒè¯å¤±è´¥ã€‚
å¤±è´¥ä½ç½®å†å²: {failed_pos_str}

é”™è¯¯åˆ†æ: éªŒè¯æ¨¡å‹æœªæä¾›æ˜ç¡®çš„ä½ç½®å»ºè®®ï¼Œè¯´æ˜å½“å‰é¢„æµ‹ä¸¥é‡åç¦»ç›®æ ‡ã€‚
æŒ‡å¯¼å»ºè®®: é‡æ–°å®¡è§†ä»»åŠ¡è¦æ±‚ï¼Œåœ¨å›¾åƒä¸­å¯»æ‰¾ç¬¦åˆä»»åŠ¡æè¿°çš„UIå…ƒç´ ï¼Œå°è¯•å®Œå…¨ä¸åŒçš„åŒºåŸŸã€‚"""
                
                current_image = marked_path
        
        print(f"â° è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œåœæ­¢å°è¯•")
        return {
            "success": False,
            "final_position": (x, y),
            "iterations": max_iterations,
            "final_image": marked_path
        }

def main():
    """æ¼”ç¤ºä½¿ç”¨"""
    print("ğŸš€ åŸºäºQwen2.5-VLçš„è¿­ä»£å¼è§†è§‰Groundingç³»ç»Ÿ")
    print("="*60)
    print("åŠŸèƒ½ï¼šå›¾åƒåˆ†æ â†’ ä½ç½®é¢„æµ‹ â†’ è‡ªæˆ‘éªŒè¯ â†’ è¿­ä»£ä¼˜åŒ–")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        grounding = QwenVL25IterativeGrounding()
        
        # æµ‹è¯•å‚æ•°
        image_path = "/home/wgb/mobile_images/0a0f9f9d-cc60-48e6-9450-c1b18f57fabc.png"
        task = "how to create the recipes?"
        
        if os.path.exists(image_path):
            print(f"âœ… æ‰¾åˆ°å›¾åƒæ–‡ä»¶: {image_path}")
            
            # è¿è¡Œè¿­ä»£grounding
            result = grounding.run_iterative_grounding(image_path, task, max_iterations=3)
            
            print("\n" + "="*50)
            print("ğŸ æœ€ç»ˆç»“æœ:")
            if result and result["success"]:
                print(f"âœ… æˆåŠŸæ‰¾åˆ°ä½ç½®: {result['final_position']}")
                print(f"ğŸ“Š è¿­ä»£æ¬¡æ•°: {result['iterations']}")
                print(f"ğŸ–¼ï¸ æœ€ç»ˆæ ‡è®°å›¾åƒ: {result['final_image']}")
            else:
                print("âŒ æœªèƒ½æ‰¾åˆ°å‡†ç¡®ä½ç½®")
                if result:
                    print(f"ğŸ“Š å°è¯•äº† {result['iterations']} æ¬¡è¿­ä»£")
                    print(f"ğŸ–¼ï¸ æœ€åçš„æ ‡è®°å›¾åƒ: {result['final_image']}")
        else:
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            print("è¯·ä¿®æ”¹image_pathä¸ºæœ‰æ•ˆçš„å›¾åƒè·¯å¾„")
            
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("1. ç¡®ä¿å·²å®‰è£…: pip install git+https://github.com/huggingface/transformers accelerate")
        print("2. ç¡®ä¿å·²å®‰è£…: pip install qwen-vl-utils")  
        print("3. ç¡®ä¿å·²å®‰è£…: pip install torch>=2.0.0")
        print("4. æ£€æŸ¥Qwen2.5-VLæ¨¡å‹æ˜¯å¦å·²ä¸‹è½½")
        print("5. ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")

if __name__ == "__main__":
    main() 